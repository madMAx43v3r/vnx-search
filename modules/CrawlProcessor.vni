package vnx.search;

module CrawlProcessor {
	
	vnx.TopicPtr input_url_index = "backend.url_index.updates";
	
	vnx.TopicPtr output_crawl_stats = "crawl.stats";
	
	string url_index_server = "UrlIndex";
	string page_index_server = "PageIndex";
	string page_content_server = "PageContent";
	string crawl_frontend_server = "CrawlFrontend";
	
	int jump_cost = 3;
	
	int max_depth = 5;
	
	int reload_interval = 10000;			// [seconds / (depth + 1) ^ reload_power]
	
	int error_reload_interval = 100000;		// in case of network failure [seconds]
	
	int sync_interval = 10000;				// checking all urls [sec]
	
	int max_per_minute = 12;				// max pages/min for a single domain
	
	int max_num_pending = 100;				// number of fetches
	
	int max_queue_size = 1000;				// number of domains
	
	int max_url_length = 256;
	
	int max_word_length = 64;
	
	int num_threads = 4;
	
	int check_interval_ms = 500;			// queue check [ms]
	
	int robots_timeout = 1000;				// max time to wait for robots.txt [sec]
	
	int robots_reload_interval = 2678400;	// [sec]
	
	int commit_delay = 100;					// [sec]
	
	int lock_timeout = 100;					// [sec]
	
	int slow_crawl_per_minute = 60;			// total max pages/min for over depth pages
	
	float reload_power = 4;
	
	uint index_version = 0;					// increase to re-process all content
	
	bool do_reprocess = false;				// if to reprocess all pages
	
	bool inititial_sync = false;			// if to perform initial url sync
	
	string user_agent = "Googlebot";		// for robots.txt matching
	
	vector<string> protocols;
	
	vector<string> root_urls;
	
	vector<string> domain_blacklist;
	
	vector<string> path_blacklist;
	
	vector<string> regex_blacklist;
	
	
	vnx.Object get_stats(int limit) const;
	
	void handle(vnx.keyvalue.SyncUpdate sample);
	
}
