package vnx.search;

module CrawlProcessor {
	
	vnx.TopicPtr input_text = "frontend.text_responses";			// TextResponse
	
	vnx.TopicPtr input_url_index = "backend.url_index.updates";
	
	vnx.TopicPtr output_crawl_stats = "backend.crawl_stats";
	
	string url_index_server = "UrlIndex";
	
	string page_index_server = "PageIndex";
	
	string page_content_server = "PageContent";
	
	string crawl_frontend_server = "CrawlFrontend";
	
	int jump_cost = 3;
	
	int max_depth = 5;
	
	int reload_interval = 10000;			// [seconds / (depth + 1) ^ reload_power]
	
	int error_reload_interval = 100000;		// in case of network failure [seconds]
	
	int sync_interval = 10000;				// checking all urls [sec]
	
	int max_per_minute = 12;				// max pages/min for a single domain
	
	int max_num_pending = 100;				// number of fetches
	
	int max_queue_size = 1000;				// number of domains
	
	int max_url_length = 256;
	
	int max_word_length = 64;
	
	int check_interval_ms = 500;			// queue check
	
	int update_interval_ms = 5000;			// queue update
	
	int robots_txt_timeout = 1000;			// max time to wait for robots.txt [sec]
	
	float reload_power = 4;
	
	uint index_version = 0;					// increase to re-process all content
	
	bool do_reprocess = false;				// if to reprocess all pages
	
	string user_agent = "Googlebot";		// for robots.txt matching
	
	string profile = "default";				// crawler config name
	
	vector<string> protocols;
	
	vector<string> root_urls;
	
	vector<string> domain_blacklist;
	
	vector<string> path_blacklist;
	
	vector<string> regex_blacklist;
	
	
	CrawlStats* get_stats(int limit) const;
	
	void handle(TextResponse sample);
	
	void handle(vnx.keyvalue.KeyValuePair sample);
	
}
